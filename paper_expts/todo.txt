We start with the German credit dataset (D2), owing the sensibility of the classification we are making which is determining one’s credit worthiness. We already have the causal relations and immutable features described in current Table 3 in the paper draft.
We will convert all the numerical features to categorical ones. I will choose some method to find the number of categories for each feature and use that consistently for all numerical features.
The dataset will be divided into 80%:20% parts. The first 80% will be used for training a classifier (I will choose a simple sklearn NN).
We will follow the sequence as followed in the example section (section 4). We start by having no constraint among features, and all features will be mutable (first experiment). There are a total of 20 features in the dataset.
The next step will be to remove the immutable features from the set of actionable features (second experiment).
The next step will be to add the age can’t decrease (third experiment).
Next, the features age, job and present employment since can’t decrease (fourth experiment).
Next, add the data manifold loss by adding the distance of 1 nearest neighbour using the KNN classifier trained on the full dataset. This loss will be multiplied by lambda, which we will vary and study its effect (fifth experiment).
For each of the 5 experiments above, we will measure these metrics:
Was the counterfactual state reached?
Path length for the original datapoint to reach the counterfactual state.
Distance to the data manifold.
Cost (which is negative reward) for the final, counterfactual state.
Time to reach the counterfactual state.
All the experiments will be measured for the datapoints in the 20% of the dataset which was not used for training, and the datapoints received a prediction of 0 which is undesirable. We can show avg statistics and also run the 5 examples above with help of one specific datapoint.